# source .venv/bin/activate
# streamlit run app.py

import streamlit as st
import pickle
import re
import sys
import os

# Add parent directory to path to import model files
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Page config
st.set_page_config(page_title="Ø£Ø¯Ø§Ø© ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù†ÙØ³ÙŠØ© Ù„Ù„Ø·Ù„Ø§Ø¨", layout="wide")

# Load model function
@st.cache_resource
def load_model():
    try:
        model_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'saved_model')
        with open(os.path.join(model_path, 'classifier.pkl'), 'rb') as f:
            classifier = pickle.load(f)
        with open(os.path.join(model_path, 'vectorizer.pkl'), 'rb') as f:
            vectorizer = pickle.load(f)
        return classifier, vectorizer
    except:
        return None, None

# Preprocess Arabic text
def preprocess_arabic_text(text):
    if not text or text.strip() == "":
        return ""
    text = re.sub(r'[^\u0600-\u06FF\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    arabic_diacritics = re.compile(r'[\u064B-\u0652\u0670\u0640]')
    text = arabic_diacritics.sub('', text)
    return text

# Predict mental health risk
def predict_mental_health_risk(note, classifier, vectorizer):
    if not note or note.strip() == "":
        return "none", 1.0, {"none": 1.0, "low": 0.0, "moderate": 0.0, "high": 0.0}

    clean_note = preprocess_arabic_text(note)
    if not clean_note:
        return "none", 1.0, {"none": 1.0, "low": 0.0, "moderate": 0.0, "high": 0.0}

    note_tfidf = vectorizer.transform([clean_note])
    prediction = classifier.predict(note_tfidf)[0]
    probabilities = classifier.predict_proba(note_tfidf)[0]
    confidence = max(probabilities)

    class_probs = dict(zip(classifier.classes_, probabilities))
    return prediction, confidence, class_probs

# Header
st.markdown(
    """
    <div style="text-align:center; padding:20px; background-color:#d73527; border-radius:12px;">
        <h1 style="color:white; margin-bottom:5px;">ğŸ§  Ø£Ø¯Ø§Ø© ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù†ÙØ³ÙŠØ© Ù„Ù„Ø·Ù„Ø§Ø¨</h1>
        <p style="color:white; font-size:16px;">ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø·Ù„Ø§Ø¨ Ø§Ù„Ù…Ø¹Ø±Ø¶ÙŠÙ† Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„ØµØ­Ø© Ø§Ù„Ù†ÙØ³ÙŠØ© ÙˆØªÙ‚Ø¯ÙŠÙ… Ø§Ù„ØªØ¯Ø®Ù„ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨</p>
    </div>
    """,
    unsafe_allow_html=True
)

st.write("")

# Important notice
# st.markdown(
#     """
#     <div style="background-color:#fff3cd; padding:15px; border-radius:8px; border-left:5px solid #ffc107;">
#         <h4 style="color:#856404; margin-bottom:10px;">ğŸ“‹ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…</h4>
#         <p style="color:#856404; margin-bottom:5px;">â€¢ Ø£Ø¯Ø®Ù„ <strong>ÙÙ‚Ø· Ø§Ù„Ù…ÙˆØ§Ù‚Ù Ø§Ù„Ù…Ø«ÙŠØ±Ø© Ù„Ù„Ù‚Ù„Ù‚</strong> Ø£Ùˆ Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ§Øª Ø§Ù„Ù…Ø´ÙƒÙˆÙƒ ÙÙŠÙ‡Ø§</p>
#         <p style="color:#856404; margin-bottom:5px;">â€¢ Ø§ØªØ±Ùƒ Ø§Ù„Ø­Ù‚Ù„ ÙØ§Ø±ØºØ§Ù‹ Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù…Ø®Ø§ÙˆÙ</p>
#         <p style="color:#856404; margin-bottom:0px;">â€¢ Ù„Ø§ ØªØ¯Ø®Ù„ ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© Ø£Ùˆ Ø¹Ø§Ø¯ÙŠØ© Ø¹Ù† Ø§Ù„Ø·Ø§Ù„Ø¨</p>
#     </div>
#     """,
#     unsafe_allow_html=True
# )

st.write("")

# Input section
col1, col2 = st.columns([1, 2])

with col1:
    st.markdown("### ğŸ†” Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø·Ø§Ù„Ø¨")
    student_id = st.text_input("Ø±Ù‚Ù… Ø§Ù„Ø·Ø§Ù„Ø¨:")

with col2:
    st.markdown("### âš ï¸ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù…Ø«ÙŠØ±Ø© Ù„Ù„Ù‚Ù„Ù‚ ÙÙ‚Ø·")
    prompt = st.text_area(
        "Ø£Ø¯Ø®Ù„ Ø£ÙŠ Ø³Ù„ÙˆÙƒ Ø£Ùˆ Ù…ÙˆÙ‚Ù Ù…Ø«ÙŠØ± Ù„Ù„Ù‚Ù„Ù‚ (Ø§ØªØ±ÙƒÙ‡ ÙØ§Ø±ØºØ§Ù‹ Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù…Ø®Ø§ÙˆÙ):",
        height=150,
        placeholder="Ù…Ø«Ø§Ù„: Ø§Ù„Ø·Ø§Ù„Ø¨ ÙŠØ¨Ø¯Ùˆ Ø­Ø²ÙŠÙ†Ø§Ù‹ Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø±ØŒ Ø£Ùˆ ÙŠØªØ¬Ù†Ø¨ Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†ØŒ Ø£Ùˆ ÙŠØ¸Ù‡Ø± ØªØºÙŠØ±Ø§Øª Ø³Ù„ÙˆÙƒÙŠØ© Ù…ÙØ§Ø¬Ø¦Ø©..."
    )

st.write("")

# Submit button
submit = st.button("ğŸ” ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù†ÙØ³ÙŠØ©", key="submit")

# Load model
classifier, vectorizer = load_model()

# AI Response area
if submit:
    if not student_id:
        st.warning("âš ï¸ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø±Ù‚Ù… Ø§Ù„Ø·Ø§Ù„Ø¨.")
    else:
        if classifier is None:
            st.error("âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.")
        else:
            # Get prediction
            prediction, confidence, probabilities = predict_mental_health_risk(prompt, classifier, vectorizer)

            # Color coding for risk levels
            risk_colors = {
                "none": "#28a745",     # Green
                "low": "#ffc107",      # Yellow
                "moderate": "#fd7e14", # Orange
                "high": "#dc3545"      # Red
            }

            risk_labels = {
                "none": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø®Ø§Ø·Ø±",
                "low": "Ù…Ø®Ø§Ø·Ø± Ù…Ù†Ø®ÙØ¶Ø©",
                "moderate": "Ù…Ø®Ø§Ø·Ø± Ù…ØªÙˆØ³Ø·Ø©",
                "high": "Ù…Ø®Ø§Ø·Ø± Ø¹Ø§Ù„ÙŠØ©"
            }

            risk_recommendations = {
                "none": "âœ… Ù„Ø§ ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ø·Ø§Ù„Ø¨ Ù„ØªØ¯Ø®Ù„ Ø®Ø§Øµ Ø­Ø§Ù„ÙŠØ§Ù‹. Ø§Ø³ØªÙ…Ø± ÙÙŠ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©.",
                "low": "ğŸŸ¡ Ø±Ø§Ù‚Ø¨ Ø§Ù„Ø·Ø§Ù„Ø¨ ÙˆØ§Ø­Ø±Øµ Ø¹Ù„Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø¥Ø¶Ø§ÙÙŠ ÙˆØ§Ù„ØªØ´Ø¬ÙŠØ¹.",
                "moderate": "ğŸŸ  ÙŠÙÙ†ØµØ­ Ø¨Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø£ÙˆÙ„ÙŠØ§Ø¡ Ø§Ù„Ø£Ù…ÙˆØ± ÙˆØ§Ù„Ø£Ø®ØµØ§Ø¦ÙŠ Ø§Ù„Ù†ÙØ³ÙŠ Ø§Ù„Ù…Ø¯Ø±Ø³ÙŠ.",
                "high": "ğŸ”´ ØªØ¯Ø®Ù„ ÙÙˆØ±ÙŠ Ù…Ø·Ù„ÙˆØ¨! Ø§ØªØµÙ„ Ø¨Ø§Ù„Ø£Ø®ØµØ§Ø¦ÙŠ Ø§Ù„Ù†ÙØ³ÙŠ ÙˆØ£ÙˆÙ„ÙŠØ§Ø¡ Ø§Ù„Ø£Ù…ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ÙÙˆØ±."
            }

            selected_color = risk_colors[prediction]
            selected_label = risk_labels[prediction]
            selected_recommendation = risk_recommendations[prediction]

            # Display results
            with st.container():
                st.markdown(
                    f"""
                    <div style="
                        background-color:#f8f9fa;
                        padding:25px;
                        border-radius:12px;
                        border-left:5px solid {selected_color};
                        box-shadow: 2px 2px 12px rgba(0,0,0,0.1);
                    ">
                        <h3 style="color:{selected_color};">ï¿½ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù†ÙØ³ÙŠØ© Ù„Ù„Ø·Ø§Ù„Ø¨ Ø±Ù‚Ù…: {student_id}</h3>

                        <div style="background-color:white; padding:15px; border-radius:8px; margin:15px 0;">
                            <h4 style="color:{selected_color}; margin-bottom:10px;">Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ø®Ø§Ø·Ø±: {selected_label}</h4>
                            <p style="font-size:16px; margin-bottom:10px;"><strong>Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤:</strong> {confidence:.1%}</p>

                            <div style="margin:15px 0;">
                                <h5>ØªÙˆØ²ÙŠØ¹ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø®Ø§Ø·Ø±:</h5>
                                <ul style="list-style-type:none; padding-left:0;">
                                    <li>ğŸŸ¢ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø®Ø§Ø·Ø±: {probabilities.get('none', 0):.1%}</li>
                                    <li>ğŸŸ¡ Ù…Ø®Ø§Ø·Ø± Ù…Ù†Ø®ÙØ¶Ø©: {probabilities.get('low', 0):.1%}</li>
                                    <li>ğŸŸ  Ù…Ø®Ø§Ø·Ø± Ù…ØªÙˆØ³Ø·Ø©: {probabilities.get('moderate', 0):.1%}</li>
                                    <li>ğŸ”´ Ù…Ø®Ø§Ø·Ø± Ø¹Ø§Ù„ÙŠØ©: {probabilities.get('high', 0):.1%}</li>
                                </ul>
                            </div>
                        </div>

                        <div style="background-color:#e9ecef; padding:15px; border-radius:8px;">
                            <h5 style="color:#495057; margin-bottom:10px;">ğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª:</h5>
                            <p style="color:#495057; margin-bottom:0px;">{selected_recommendation}</p>
                        </div>
                    </div>
                    """,
                    unsafe_allow_html=True
                )

            # Add additional resources for high risk cases
            if prediction == "high":
                st.markdown(
                    """
                    <div style="background-color:#f8d7da; padding:20px; border-radius:8px; border:1px solid #f5c6cb; margin-top:20px;">
                        <h4 style="color:#721c24;">ğŸš¨ Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø·ÙˆØ§Ø±Ø¦</h4>
                        <p style="color:#721c24;"><strong>Ø®Ø· Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù†ÙØ³ÙŠØ©:</strong> 920033360</p>
                        <p style="color:#721c24;"><strong>Ø§Ù„Ø·ÙˆØ§Ø±Ø¦:</strong> 997</p>
                        <p style="color:#721c24;"><strong>ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØµØ­Ø©:</strong> 937</p>
                    </div>
                    """,
                    unsafe_allow_html=True
                )

# Footer with disclaimer
st.markdown("---")
st.markdown(
    """
    <div style="text-align:center; padding:10px; color:#6c757d; font-size:12px;">
        <p><strong>ØªÙ†Ø¨ÙŠÙ‡:</strong> Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø¯Ø§Ø© Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙ‚Ø· ÙˆÙ„Ø§ ØªØºÙ†ÙŠ Ø¹Ù† Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ù†ÙØ³ÙŠØ© Ø§Ù„Ù…ØªØ®ØµØµØ©</p>
        <p>ÙÙŠ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø·ÙˆØ§Ø±Ø¦ØŒ Ø§ØªØµÙ„ ÙÙˆØ±Ø§Ù‹ Ø¨Ø§Ù„Ø£Ø®ØµØ§Ø¦ÙŠ Ø§Ù„Ù†ÙØ³ÙŠ Ø£Ùˆ Ø§Ù„Ø·ÙˆØ§Ø±Ø¦</p>
    </div>
    """,
    unsafe_allow_html=True
)








# import streamlit as st
# from langchain.prompts import PromptTemplate
# from langchain.chains import LLMChain
# from langchain_openai import ChatOpenAI  # âœ… New import

# st.title("Staff Decision-Support Tool")
# st.divider()
# st.write("""Enter your observations or concerns about a student.
#          The AI will suggest possible referral pathways to NCMH and ways to support the student.""")

# prompt = st.text_input("Your prompt:")
# st.divider()

# if prompt:
#     st.balloons()

    # # âœ… Use ChatOpenAI instead of old OpenAI
    # llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.3)

    # # Prompt template
    # template = """You are an assistant helping teachers identify wellness issues.
    # Given a teacher's observation about a student, suggest:
    # 1. Possible referral pathways to the National Center for Mental Health (NCMH).
    # 2. Supportive actions the teacher/school can take immediately.
    # Keep responses clear, empathetic, and professional.

    # Teacher's note: {user_prompt}"""

    # prompt_template = PromptTemplate(input_variables=["user_prompt"], template=template)

    # # Build chain
    # llm_chain = LLMChain(prompt=prompt_template, llm=llm)

    # # Run chain with teacherâ€™s prompt
    # response = llm_chain.run(prompt)

    # st.subheader("Assistant Response:")
    # st.write(response)
